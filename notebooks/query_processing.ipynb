{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX512 support.\n",
      "INFO:faiss.loader:Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 valid files\n",
      "Warning: Failed to load FAISS index: 'bytes' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from src.vector import EmbeddingManager, OpenAIEmbeddingProvider, FAISSVectorStore, EmbeddingConfig\n",
    "from src.vector import create_file_content_map\n",
    "import os\n",
    "\n",
    "file_content_map = create_file_content_map(\n",
    "    root_dir=os.path.join(\"..\", \"src\", \"vector\"), accept_patterns=[\"*.py\"])\n",
    "\n",
    "\n",
    "embedding_provider = OpenAIEmbeddingProvider(\n",
    "    config=EmbeddingConfig(model_name=\"text-embedding-3-small\"))\n",
    "vector_store = FAISSVectorStore(dimension=1536, index_path=\"faiss_index.bin\")\n",
    "\n",
    "embedding_manager = EmbeddingManager(\n",
    "    embedding_provider=embedding_provider,\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "\n",
    "texts = list(file_content_map.values())\n",
    "metadatas = [{\"source\": file_path} for file_path in file_content_map.keys()]\n",
    "\n",
    "await embedding_manager.add_texts(texts=texts, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "0 6\n",
      "AnalysisBatch(files=[WindowsPath('C:/Users/Utkarsh Priyadarshi/WebstormProjects/gep/src/vector/vector.py')], contents=['\\nFile: C:\\\\Users\\\\Utkarsh Priyadarshi\\\\WebstormProjects\\\\gep\\\\src\\\\vector\\\\vector.py\\nContent:\\n    import os\\n    import faiss\\n    import numpy as np\\n    import asyncio\\n    import aiofiles\\n    from abc import ABC, abstractmethod\\n    from dataclasses import dataclass\\n    from enum import Enum, auto\\n    from typing import Any, Optional\\n    from sqlalchemy import create_engine\\n    from sqlalchemy.orm import sessionmaker\\n    from langchain_core.embeddings import Embeddings\\n    from langchain_openai import OpenAIEmbeddings\\n    from langchain_community.vectorstores import Chroma, FAISS, PGVector, Pinecone\\n    @dataclass\\n    class EmbeddingVector:\\n    id: str\\n    vector: list[float]\\n    metadata: dict[str, Any]\\n    text: str\\n    @dataclass\\n    class SearchResult:\\n    text: str\\n    metadata: dict[str, Any]\\n    score: float\\n        vector_id: str\\n            source_file: str\\n    class VectorStoreType(Enum):\\n    CHROMA = auto()\\n    PGVECTOR = auto()\\n    FAISS = auto()\\n    PINECONE = auto()\\n        @dataclass\\n        class VectorStoreConfig:\\n    store_type: VectorStoreType\\n    connection_params: dict[str, Any]\\n        embedding_model: Optional[Embeddings] = None\\n        def __post_init__(self):\\n    if self.embedding_model is None:\\n    self.embedding_model = OpenAIEmbeddings()\\n        class VectorStore(ABC):\\n        @abstractmethod\\n    async def add_vectors(self, vectors: list[EmbeddingVector]) -> None:\\n    pass\\n        @abstractmethod\\n        async def query(self, query_vector: list[float], k: int = 10, filter: Optional[dict] = None) -> list[SearchResult]:\\n    pass\\n    @abstractmethod\\n    async def delete(self, ids: list[str]) -> None:\\n    pass\\n        @abstractmethod\\n        async def clear(self) -> None:\\n        pass\\n                            class ChromaVectorStore(VectorStore):\\n    def __init__(self, persist_dir: str, embedding_model: Optional[Embeddings] = None):\\n        self.persist_dir = persist_dir\\n        self.embedding_model = embedding_model or OpenAIEmbeddings()\\n            self.store = Chroma(persist_directory=persist_dir,\\n            embedding_function=self.embedding_model)\\n            def add_vectors(self, vectors: list[EmbeddingVector]) -> None:\\n            try:\\n            documents = [v.text for v in vectors]\\n                metadatas = [v.metadata for v in vectors]\\n                embeddings = [v.vector for v in vectors]\\n                ids = [v.id for v in vectors]\\n                self.store.add_documents(\\n            documents=documents,\\n        metadatas=metadatas,\\n            embeddings=embeddings,\\n    ids=ids\\n        )\\n        except Exception as e:\\n            raise RuntimeError(f\"Failed to add vectors to Chroma: {e}\")\\n        def clear(self) -> None:\\n            try:\\n    self.store.delete_collection()\\n        except Exception as e:\\n        raise RuntimeError(f\"Failed to clear Chroma store: {e}\")\\n            def delete(self, ids: list[str]) -> None:\\n        try:\\n            self.store.delete(ids)\\n    except Exception as e:\\n    raise RuntimeError(f\"Failed to delete vectors from Chroma: {e}\")\\n    class FAISSVectorStore(VectorStore):\\n    def __init__(self, dimension: int, index_path: str, embedding_model: Optional[Embeddings] = None) -> None:\\n        self.dimension = dimension\\n        self.index_path = index_path\\n        self.embedding_model = embedding_model or OpenAIEmbeddings()\\n        self.index = faiss.IndexFlatL2(dimension)\\n        self.id_map: dict[str, int] = {}\\n        self.vectors: list[EmbeddingVector] = []\\n        asyncio.create_task(self.__load_index())\\n    async def query(self, query_vector: list[float], k: int = 10, filter: Optional[dict] = None) -> list[SearchResult]:\\n        try:\\n        query_arr = np.array([query_vector], dtype=np.float32)\\n            distances, indices = self.index.search(query_arr, k)\\n            results = []\\n            for distance, idx in zip(distances[0], indices[0]):\\n            if idx < 0 or idx >= len(self.vectors):\\n                continue\\n                    vector = self.vectors[idx]\\n                results.append(SearchResult(\\n                text=vector.text,\\n                    metadata=vector.metadata,\\n                    score=float(distance),\\n                    vector_id=vector.id,\\n                    source_file=vector.metadata[\"source\"]\\n                    ))\\n                return results\\n            except Exception as e:\\n        raise RuntimeError(f\"Failed to query FAISS: {str(e)}\") from e\\n            async def __load_index(self) -> None:\\n    try:\\n        idx_path = self.index_path\\n        if os.path.exists(idx_path):\\n            async with aiofiles.open(idx_path, \\'rb\\') as f:\\n            index_data = await f.read()\\n                self.index = await asyncio.to_thread(\\n                    faiss.deserialize_index, index_data\\n                )\\n                    except Exception as e:\\n                print(f\"Warning: Failed to load FAISS index: {e}\")\\n        async def __save_index(self) -> None:\\n            try:\\n    index_data = await asyncio.to_thread(faiss.serialize_index, self.index)\\n        async with aiofiles.open(self.index_path, \\'wb\\') as f:\\n        await f.write(index_data)\\n            except Exception as e:\\n            print(f\"Warning: Failed to save FAISS index: {e}\")\\n                async def add_vectors(self, vectors: list[EmbeddingVector]) -> None:\\n        try:\\n            if not vectors:\\n    return\\n        vec_arr = np.array(\\n        [v.vector for v in vectors], dtype=np.float32)\\n            start_idx = len(self.vectors)\\n                self.index.add(vec_arr)\\n            for i, vector in enumerate(vectors):\\n                self.id_map[vector.id] = start_idx + i\\n            self.vectors.append(vector)\\n            await self.__save_index()\\n            except Exception as e:\\n                raise RuntimeError(f\"Failed to add vectors to FAISS: {e}\")\\n                async def delete(self, ids: list[str]) -> None:\\n            try:\\n        indices_to_delete = [self.id_map[id] for id in ids]\\n            if not indices_to_delete:\\n    return\\n        new_vectors = []\\n        new_id_map = {}\\n            for i, vector in enumerate(self.vectors):\\n            if i not in indices_to_delete:\\n                new_id_map[vector.id] = len(new_vectors)\\n            new_vectors.append(vector)\\n            self.index = faiss.IndexFlatL2(self.dimension)\\n            if new_vectors:\\n                vectors_array = np.array(\\n                    [v.vector for v in new_vectors], dtype=np.float32)\\n                    self.index.add(vectors_array)\\n            self.vectors = new_vectors\\n            self.id_map = new_id_map\\n            await self.__save_index()\\n                except Exception as e:\\n                    raise RuntimeError(f\"Failed to delete vectors from FAISS: {e}\")\\n                async def clear(self) -> None:\\n            try:\\n            self.index = faiss.IndexFlatL2(self.dimension)\\n            self.id_map = {}\\n        self.vectors = []\\n            await self.__save_index()\\n    except Exception as e:\\n        raise RuntimeError(f\"Failed to clear FAISS store: {e}\")\\n        class PGVectorStore(VectorStore):\\n            def __init__(self, connection_str: str, table_name: str = \"embeddings\", embedding_model: Optional[Embeddings] = None):\\n            self.connection_str = connection_str\\n            self.table_name = table_name\\n            self.embedding_model = embedding_model or OpenAIEmbeddings()\\n        self.engine = create_engine(connection_str)\\n            self.session = sessionmaker(bind=self.engine)\\n    with self.engine.connect() as conn:\\n    conn.execute(f)\\n    conn.execute(f)\\n    def _initialize_store(self) -> PGVector:\\n        return PGVector(embedding=self.config.embedding_model)\\n        \\n'], total_chars=7699, metadata={'end_index': 1, 'start_index': 0, 'total_results': 6})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6\n",
      "AnalysisBatch(files=[WindowsPath('C:/Users/Utkarsh Priyadarshi/WebstormProjects/gep/src/vector/manager.py'), WindowsPath('C:/Users/Utkarsh Priyadarshi/WebstormProjects/gep/src/vector/__init__.py')], contents=['\\nFile: C:\\\\Users\\\\Utkarsh Priyadarshi\\\\WebstormProjects\\\\gep\\\\src\\\\vector\\\\manager.py\\nContent:\\n    import time\\n    from typing import Optional\\n    from .embedding import EmbeddingProvider\\n    from .vector import VectorStore, EmbeddingVector, SearchResult\\n        class EmbeddingManager:\\n        def __init__(self, embedding_provider: EmbeddingProvider, vector_store: VectorStore, batch_size: int = 100) -> None:\\n        self.embedding_provider = embedding_provider\\n            self.batch_size = batch_size\\n            self.vector_store = vector_store\\n            async def add_texts(self, texts: list[str], metadatas: Optional[list[dict]] = None) -> None:\\n        try:\\n        embeddings = await self.embedding_provider.embed_documents(texts)\\n        vectors = []\\n        for i, (text, embedding) in enumerate(zip(texts, embeddings)):\\n    md = metadatas[i] if metadatas else {}\\n        vector = EmbeddingVector(\\n        id=f\"vector_{time.time()}_{i}\",\\n        vector=embedding,\\n        metadata=md,\\n            text=text,\\n            )\\n            vectors.append(vector)\\n                await self.vector_store.add_vectors(vectors)\\n                except Exception as e:\\n                    print(f\"Error adding texts to vector store: {e}\")\\n                    raise e\\n                    async def delete_vectors(self, ids: list[str]) -> None:\\n                    await self.vector_store.delete(ids)\\n                async def similarity_search(self, query: str, limit: int = 10, filter: Optional[dict] = None) -> list[SearchResult]:\\n                try:\\n            query_embedding = await self.embedding_provider.embed_query(query)\\n        return await self.vector_store.query(\\n            query_vector=query_embedding,\\n            k=limit,\\n    filter=filter,\\n        )\\n        except Exception as e:\\n        print(f\"Error performing similarity search: {e}\")\\n        raise e\\n    \\n', '\\nFile: C:\\\\Users\\\\Utkarsh Priyadarshi\\\\WebstormProjects\\\\gep\\\\src\\\\vector\\\\__init__.py\\nContent:\\n    from .manager import EmbeddingManager, EmbeddingProvider\\n    from .embedding import EmbeddingConfig, create_embedding_provider, EmbeddingProviderType, OpenAIEmbeddingProvider\\n    from .vector import VectorStore, VectorStoreConfig, VectorStoreType, ChromaVectorStore, PGVectorStore, FAISSVectorStore, EmbeddingVector, SearchResult\\n    from .file import FileMetadata, FileEvent, EventType, FileManager, FileManagerConfig\\n    from .utils import create_file_content_map\\n    __all__ = [\\n    \"EmbeddingManager\",\\n    \"EmbeddingProvider\",\\n    \"EmbeddingConfig\",\\n    \"create_embedding_provider\",\\n    \"EmbeddingProviderType\",\\n    \"OpenAIEmbeddingProvider\",\\n    \"VectorStore\",\\n    \"VectorStoreConfig\",\\n    \"VectorStoreType\",\\n    \"ChromaVectorStore\",\\n    \"FileMetadata\",\\n    \"FileEvent\",\\n    \"EventType\",\\n    \"FileManager\",\\n \"FileManagerConfig\",\\n \"PGVectorStore\",\\n \"FAISSVectorStore\",\\n \"EmbeddingVector\",\\n \"create_file_content_map\",\\n \"SearchResult\",\\n]\\n\\n'], total_chars=2909, metadata={'end_index': 3, 'start_index': 1, 'total_results': 6})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 6\n",
      "AnalysisBatch(files=[WindowsPath('C:/Users/Utkarsh Priyadarshi/WebstormProjects/gep/src/vector/file.py')], contents=['\\nFile: C:\\\\Users\\\\Utkarsh Priyadarshi\\\\WebstormProjects\\\\gep\\\\src\\\\vector\\\\file.py\\nContent:\\n    import asyncio\\n    import logging\\n    import time\\n    import aiofiles\\n    import hashlib\\n    import pathlib\\n    from dataclasses import dataclass, field\\n    from .manager import EmbeddingManager\\n    from src.watcher import AsyncFileWatcher, default_ignore_patterns, FileEvent, EventType\\n    from langchain_text_splitters import RecursiveCharacterTextSplitter\\n    logging.basicConfig(level=logging.INFO)\\n    logger = logging.getLogger(__name__)\\n    @dataclass\\n    class FileMetadata:\\n        file_path: str\\n    last_modified: float\\n    content_hash: str\\n    vector_ids: list[str]\\n    chunks: list[str]\\n    is_processed: bool = False\\n        @dataclass\\n        class FileManagerConfig:\\n        batch_size: int = 10\\n            ignored_patterns: list[str] = field(\\n            default_factory=lambda: default_ignore_patterns)\\n        class FileManager:\\n        def __init__(self, root_path: str, config: FileManagerConfig, embedding_manager: EmbeddingManager) -> None:\\n        self.root_path = pathlib.Path(root_path).resolve()\\n        self.config = config\\n        self.file_watcher = AsyncFileWatcher(\\n        root_path=str(self.root_path),\\n        ignored_patterns=self.config.ignored_patterns,\\n            )\\n    self.embedding_manager = embedding_manager\\n        self.processed_files: dict[str, FileMetadata] = {}\\n        self._process_lock = asyncio.Lock()\\n        self.is_running = False\\n        self.batch_queue: asyncio.Queue[str] = asyncio.Queue(maxsize=1000)\\n            self.text_splitter = RecursiveCharacterTextSplitter(\\n            chunk_size=1000, chunk_overlap=200)\\n                async def __handle_file_event(self, event: FileEvent) -> None:\\n                    try:\\n                    ev_type = event.event_type\\n                    if ev_type == EventType.FILE_DELETED:\\n            if event.path in self.processed_files:\\n                file_info = self.processed_files[event.path]\\n                print(f\"File deleted: {event.path}\")\\n        del self.processed_files[event.path]\\n            else:\\n    await self.batch_queue.put(event)\\n        logger.info(f\"Queued file event: {event.path}\")\\n            except Exception as e:\\n        logger.error(f\"Error handling file event: {e}\")\\n        async def start(self) -> None:\\n            if self.is_running:\\n            raise RuntimeError(\"File manager is already running\")\\n            self.is_running = True\\n            try:\\n                await self.file_watcher.watch()\\n        await asyncio.create_task(self.__batch_processor())\\n            await self.__process_existing_files()\\n            async for event in self.file_watcher.get_event():\\n    await self.__handle_file_event(event)\\n        except Exception as e:\\n        logger.error(f\"Error starting file manager: {e}\")\\n        self.is_running = False\\n        async def stop(self) -> None:\\n        logger.info(\"Stopping integrated embedding manager...\")\\n        self.is_running = False\\n            try:\\n            await self.file_watcher.stop()\\n                while not self.batch_queue.empty():\\n                file_path = await self.batch_queue.get()\\n            await self.__process_file(file_path)\\n            await self.batch_queue.join()\\n        logger.info(\"File manager stopped successfully\")\\n            except Exception as e:\\n    logger.error(f\"Error stopping file manager: {e}\")\\n        async def __process_existing_files(self) -> None:\\n        logger.info(\"Processing existing files...\")\\n        for file_path in self.root_path.rglob(\"*\"):\\n        if file_path.is_file():\\n        await self.batch_queue.put(str(file_path))\\n            async def __batch_processor(self) -> None:\\n                while self.is_running:\\n    try:\\n        batch: list[str] = []\\n        while len(batch) < self.config.batch_size:\\n        try:\\n        file_path = await asyncio.wait_for(self.batch_queue.get(), timeout=1.0)\\n            batch.append(file_path)\\n                except asyncio.TimeoutError:\\n                break\\n                    if batch:\\n                        await asyncio.gather(*(self.__process_file(file_path) for file_path in batch))\\n                        except Exception as e:\\n                    logger.error(f\"Error processing batch: {e}\")\\n                        await asyncio.sleep(1.0)\\n                @staticmethod\\n                    async def __read_file(file_path: str) -> str:\\n            async with aiofiles.open(file_path, \"r\", encoding=\"utf-8\") as file:\\n                return await file.read()\\n                @staticmethod\\n    def __hash_content(content: str) -> str:\\n    return hashlib.sha256(content.encode()).hexdigest()\\n        async def __process_file(self, file_path: str) -> None:\\n        async with self._process_lock:\\n        if file_path in self.processed_files:\\n        if self.processed_files[file_path].is_processed:\\n            logger.info(f\"File already processed: {file_path}\")\\n    return\\n    try:\\n        file_info = FileMetadata(\\n        file_path=file_path,\\n        last_modified=time.time(),\\n        content_hash=\"\",\\n    is_processed=True,\\n        )\\n        content = await self.__read_file(file_path)\\n        content_hash = self.__hash_content(content)\\n        if (file_path in self.processed_files and\\n        self.processed_files[file_path].content_hash == content_hash):\\n            logger.debug(\\n                f\"File {file_path} content unchanged, skipping\")\\n                    return\\n                    vector_ids = await self.__embed(content, file_info)\\n            file_info.vector_ids = vector_ids\\n                file_info.content_hash = content_hash\\n                    file_info.is_processed = False\\n                    self.processed_files[file_path] = file_info\\n                    except Exception as e:\\n                    logger.error(f\"Error processing file {file_path}: {e}\")\\n                if file_path in self.processed_files:\\n                del self.processed_files[file_path]\\n                def __chunk_content(self, content: str) -> list[str]:\\n                try:\\n                        return self.text_splitter.split_text(content)\\n                    except Exception as e:\\n                        logger.error(f\"Error splitting content: {e}\")\\n                    return [content]\\n                async def __embed(self, file_content: str, metadata: FileMetadata) -> list[str]:\\n                chunks = self.__chunk_content(file_content)\\n                file_path = metadata.file_path\\n                chunks_metadata = [{\\n                \"source\": file_path,\\n            \"chunk_idx\": i,\\n                \"total_chunks\": len(chunks),\\n                \"timestamp\": time.time(),\\n                    \"file_type\": pathlib.Path(file_path).suffix,\\n    \"relative_path\": str(pathlib.Path(file_path).relative_to(self.root_path))\\n        } for i, chunk in enumerate(chunks)]\\n        if file_path in self.processed_files:\\n        old_vectors = self.processed_files[file_path].vector_ids\\n        if old_vectors:\\n            await self.embedding_manager.delete_vectors(old_vectors)\\n        return await self.embedding_manager.add_vectors(chunks, chunks_metadata)\\n            \\n'], total_chars=7159, metadata={'end_index': 4, 'start_index': 3, 'total_results': 6})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 6\n",
      "AnalysisBatch(files=[WindowsPath('C:/Users/Utkarsh Priyadarshi/WebstormProjects/gep/src/vector/embedding.py'), WindowsPath('C:/Users/Utkarsh Priyadarshi/WebstormProjects/gep/src/vector/utils.py')], contents=['\\nFile: C:\\\\Users\\\\Utkarsh Priyadarshi\\\\WebstormProjects\\\\gep\\\\src\\\\vector\\\\embedding.py\\nContent:\\n    from abc import ABC, abstractmethod\\n    from typing import Optional\\n    from enum import Enum, auto\\n    import logging\\n    import asyncio\\n    from langchain_openai import OpenAIEmbeddings\\n        from tenacity import retry, stop_after_attempt, wait_exponential\\n        logging.basicConfig(level=logging.INFO)\\n        logger = logging.getLogger(__name__)\\n        class EmbeddingProviderType(Enum):\\n    OPENAI = auto()\\n    CLAUDE = auto()\\n    class EmbeddingConfig:\\n    def __init__(self, model_name: str, api_key: Optional[str] = None, batch_size: int = 100, **kwargs) -> None:\\n        self.model_name = model_name\\n    self.api_key = api_key\\n    self.batch_size = batch_size\\n        self.extra_config = kwargs\\n        class EmbeddingProvider(ABC):\\n        def __init__(self, config: EmbeddingConfig) -> None:\\n        self.config = config\\n    @abstractmethod\\n    async def embed_documents(self, documents: list[str]) -> list[list[float]]:\\n        pass\\n        @abstractmethod\\n        async def embed_query(self, query: str) -> list[float]:\\n        pass\\n    class OpenAIEmbeddingProvider(EmbeddingProvider):\\n    def __init__(self, config: EmbeddingConfig) -> None:\\n    super().__init__(config)\\n    self.client = OpenAIEmbeddings(\\n        model=config.model_name,\\n        openai_api_key=config.api_key,\\n            **config.extra_config,\\n            )\\n            @retry(\\n        stop=stop_after_attempt(3),\\n    wait=wait_exponential(multiplier=1, min=4, max=10),\\n        )\\n        async def embed_documents(self, documents: list[str]) -> list[list[float]]:\\n    batch_size = self.config.batch_size\\n    all_embeddings = []\\n        try:\\n        for i in range(0, len(documents), batch_size):\\n        batch = documents[i: i + batch_size]\\n        embeddings = await self.client.aembed_documents(batch)\\n        all_embeddings.extend(embeddings)\\n        if i + batch_size < len(documents):\\n            await asyncio.sleep(0.1)\\n                except Exception as e:\\n                logger.error(f\"Error embedding documents: {e}\")\\n                raise e\\n                return all_embeddings\\n                    async def embed_query(self, text: str) -> list[float]:\\n        try:\\n            return await self.client.aembed_query(text)\\n            except Exception as e:\\n        logger.error(f\"Error generating query embedding: {str(e)}\")\\n    raise\\n        def create_embedding_provider(provider_type: EmbeddingProviderType, config: EmbeddingConfig) -> EmbeddingProvider:\\n        if provider_type == EmbeddingProviderType.OPENAI:\\n        return OpenAIEmbeddingProvider(config)\\n        else:\\n            raise ValueError(f\"Invalid embedding provider type: {provider_type}\")\\n        \\n', '\\nFile: C:\\\\Users\\\\Utkarsh Priyadarshi\\\\WebstormProjects\\\\gep\\\\src\\\\vector\\\\utils.py\\nContent:\\n    import os\\n    from pathlib import Path\\n    from multiprocessing import Pool\\n    def create_file_content_map(root_dir: str, ignore_patterns: list[str] = [], accept_patterns: list[str] = []) -> dict[str, str]:\\n    root_dir = Path(root_dir).resolve()\\n    valid_files = []\\n    for file in root_dir.rglob(\"*\"):\\n        if file.is_dir():\\n            continue\\n        if _is_valid_file(file, ignore_patterns, accept_patterns):\\n            valid_files.append(file)\\n    print(f\"Found {len(valid_files)} valid files\")\\n    with Pool(processes=os.cpu_count()) as pool:\\n        results = pool.map(_read_file, valid_files)\\n    return dict(results)\\n    def _read_file(file_path: Path) -> tuple[Path, str]:\\n    with open(file_path, \\'r\\', encoding=\\'utf-8\\') as f:\\n        return file_path, f.read()\\n    def _is_valid_file(file: Path, ignore_patterns: list[str], accept_patterns: list[str]) -> bool:\\n    is_accepted = False\\n    if accept_patterns:\\n        is_accepted = any(file.match(pattern) for pattern in accept_patterns)\\n        if not is_accepted:\\n        return False\\n    if ignore_patterns and any(file.match(pattern) for pattern in ignore_patterns):\\n        return False\\n    return True\\n    \\n'], total_chars=4037, metadata={'end_index': 6, 'start_index': 4, 'total_results': 6})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamed 6 results and 4 batches\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"### [FILE]\\nFile Path: C:\\\\Users\\\\Utkarsh Priyadarshi\\\\WebstormProjects\\\\gep\\\\src\\\\vector\\\\vector.py\\n\\n#### Key Concepts:\\n- **Object-Oriented Programming (OOP)**: The file employs OOP principles, particularly through the use of abstract base classes (`VectorStore`) and data classes (`EmbeddingVector`, `SearchResult`, `VectorStoreConfig`).\\n- **Asynchronous Programming**: Utilizes `async` and `await` keywords for non-blocking operations, particularly for I/O-bound tasks like adding and querying vectors in storage.\\n- **Enumeration**: Defines a `VectorStoreType` enum for various vector store implementations, enhancing code clarity and maintainability.\\n\\n#### Primary Abstractions and Data Structures:\\n- **EmbeddingVector**: Represents a vector with an associated ID, metadata, and textual content.\\n- **SearchResult**: Encapsulates the result of a vector query, including the text, metadata, score, and source file.\\n- **VectorStore**: An abstract base class that defines the interface for various types of vector stores (e.g., FAISS, Chroma).\\n- **VectorStoreConfig**: A configuration data class for initializing a vector store.\\n\\n#### Core Algorithms and Techniques Implemented Here:\\n- **Vector Addition**: Methods to add vectors to the respective storage systems.\\n- **Querying**: Mechanisms to retrieve similar vectors based on a query vector.\\n- **Index Management**: Loading and saving vector indexes in a persistent manner (especially in FAISS).\\n- **Error Handling**: Each method contains try-except blocks to manage exceptions and provide informative error messages.\\n\\n### Code Flow:\\n1. **Initialization**:\\n   - The `VectorStoreConfig` class initializes an embedding model if none is provided.\\n   - The `VectorStore` abstract class defines the core methods (`add_vectors`, `query`, `delete`, `clear`) that concrete implementations must define.\\n\\n2. **Vector Addition**:\\n   - In concrete implementations (like `ChromaVectorStore` and `FAISSVectorStore`), the `add_vectors` method processes incoming vectors, converts them into the required format, and adds them to the storage.\\n\\n3. **Querying**:\\n   - The `query` method in `FAISSVectorStore` retrieves the nearest vectors based on a query vector and returns search results.\\n\\n4. **Deletion and Clearing**:\\n   - Methods to delete specific vectors or clear all vectors from the storage are implemented in both concrete classes.\\n\\n5. **Index Management**:\\n   - FAISS functionality includes loading and saving the index asynchronously, ensuring the index is persisted across sessions.\\n\\n### Important Control Structures:\\n- **Abstract Methods**: Define contracts for subclasses, ensuring that they implement required functionalities.\\n- **Try-Except Blocks**: Ensure robust error handling throughout the file, catching exceptions that may occur during vector operations.\\n\\n### Key Function Interactions:\\n- **VectorStore and its Subclasses**: The subclasses implement the abstract methods defined in `VectorStore`, allowing them to be interchangeable while providing specific functionalities.\\n- **Embedding Models**: The embedding model is used to generate vector representations, and each vector store can use a different embedding strategy.\\n\\n### Dependencies:\\n#### External Library Dependencies:\\n- **FAISS**: For efficient similarity search and clustering of dense vectors.\\n- **NumPy**: For numerical operations, particularly with arrays.\\n- **SQLAlchemy**: For database interactions (specifically in PGVectorStore).\\n- **aiofiles**: For asynchronous file operations.\\n- **langchain**: For embedding models and vector store implementations.\\n\\n#### Internal Module Dependencies:\\n- There are no visible internal module dependencies in the provided code snippet.\\n\\n#### Interfaces Implemented or Used:\\n- **Abstract Base Class (ABC)**: The `VectorStore` serves as an interface for its subclasses, defining the necessary methods for any vector store implementation.\\n\\n### Implementation Notes:\\n- **Notable Implementation Decisions**: The use of asynchronous programming is a notable choice, allowing for efficient handling of I/O operations without blocking.\\n- **File-specific Optimizations**: The `__load_index` and `__save_index` methods are designed to handle index persistence efficiently.\\n- **Error Handling**: Consistent use of try-except blocks provides clarity and helps in debugging issues related to vector operations.\\n\\n### [END_FILE]Certainly! Below is the detailed analysis of each file as per your requested structure.\\n\\n---\\n\\n[FILE]\\nFile Path: C:\\\\Users\\\\Utkarsh Priyadarshi\\\\WebstormProjects\\\\gep\\\\src\\\\vector\\\\manager.py\\n\\nKey Concepts:\\n- Asynchronous programming: The use of `async` and `await` for non-blocking operations when dealing with embeddings and vector storage.\\n- Object-oriented programming: The use of classes (`EmbeddingManager`) to encapsulate behavior and state related to embedding and vector management.\\n- Error handling: `try` and `except` blocks are used to manage exceptions during embedding and querying processes.\\n\\nPrimary Abstractions and Data Structures:\\n- `EmbeddingManager`: Manages the workflow of creating embeddings from texts and interacting with a vector store.\\n- `EmbeddingVector`: Represents a vector along with its metadata and text.\\n- `SearchResult`: Represents the result of a similarity search.\\n\\nCore Algorithms and Techniques Implemented:\\n- Embedding creation: The process of converting text into vector embeddings using an `EmbeddingProvider`.\\n- Vector addition and deletion: Methods to add vectors to or delete them from the `VectorStore`.\\n- Similarity search: A method for querying the vector store using a text-based query.\\n\\nCode Flow:\\n- Initialization (`__init__`): The `EmbeddingManager` is initialized with an `EmbeddingProvider`, a `VectorStore`, and an optional batch size.\\n- `add_texts`: Asynchronously embeds a list of texts and stores them as vectors in the `VectorStore`.\\n  - Retrieves embeddings for the texts.\\n  - Constructs `EmbeddingVector` instances and adds them to the `VectorStore`.\\n- `delete_vectors`: Asynchronously deletes specified vectors from the `VectorStore`.\\n- `similarity_search`: Asynchronously performs a similarity search in the `VectorStore` using an embedded query.\\n  - Embeds the query and retrieves results based on similarity.\\n\\nDependencies:\\n- External libraries: None explicitly mentioned, but the file may rely on asynchronous features from Python's standard library.\\n- Internal module dependencies: \\n  - `EmbeddingProvider`: Interface for generating embeddings.\\n  - `VectorStore`: Interface for storing and querying vectors.\\n  - `EmbeddingVector`: Data structure for representing vectors.\\n  - `SearchResult`: Data structure for representing search results.\\n\\nImplementation Notes:\\n- The file emphasizes asynchronous operations to enhance performance during I/O-bound tasks (embedding and storage).\\n- Error handling is implemented to catch and report issues while processing texts and performing searches.\\n- The use of time-based identifiers for vectors ensures that each vector has a unique ID.\\n\\n[END_FILE]\\n\\n---\\n\\n[FILE]\\nFile Path: C:\\\\Users\\\\Utkarsh Priyadarshi\\\\WebstormProjects\\\\gep\\\\src\\\\vector\\\\__init__.py\\n\\nKey Concepts:\\n- Module organization: This file serves as an initializer for the `vector` package, exposing key components.\\n- Encapsulation: By specifying `__all__`, the file controls which classes and functions are publicly accessible when the module is imported.\\n\\nPrimary Abstractions and Data Structures:\\n- Various classes and types are imported and made available, including `EmbeddingManager`, `VectorStore`, and specific implementations like `ChromaVectorStore`.\\n\\nCore Algorithms and Techniques Implemented:\\n- The file does not implement algorithms directly but organizes and exposes the core abstractions for managing embeddings and vector storage.\\n\\nCode Flow:\\n- Imports: The file imports various classes and functions from other modules within the `vector` package.\\n- Package exposure: It defines `__all__`, which includes a list of modules, classes, and functions that are public.\\n\\nDependencies:\\n- Internal module dependencies:\\n  - `manager`: Provides the `EmbeddingManager` and `EmbeddingProvider`.\\n  - `embedding`: Contains embedding-related classes and functions.\\n  - `vector`: Defines the `VectorStore`, `EmbeddingVector`, and related classes.\\n  - `file`: Manages files in the system.\\n  - `utils`: Contains utility functions like `create_file_content_map`.\\n\\nImplementation Notes:\\n- The file acts as a fa√ßade for the `vector` package, making it easier for users to access the module's functionalities.\\n- By using `__all__`, it prevents unintended access to internal classes or functions that are not meant for public use.\\n- The structure promotes modularity and maintainability of the codebase.\\n\\n[END_FILE]\\n\\n---\\n\\n### Summary of the VectorStore Class Purpose:\\nThe `VectorStore` class serves as an interface for storing, managing, and querying vector embeddings generated from text data. It facilitates operations like adding new vectors, deleting existing ones, and performing similarity searches based on embedded queries.[FILE]\\nFile Path: C:\\\\Users\\\\Utkarsh Priyadarshi\\\\WebstormProjects\\\\gep\\\\src\\\\vector\\\\file.py\\n\\nKey Concepts:\\n- **Asynchronous Programming**: Utilizes `asyncio` and `aiofiles` to handle file operations and events without blocking.\\n- **Data Classes**: `FileMetadata` and `FileManagerConfig` are defined using the `@dataclass` decorator for easy data storage.\\n- **Event-Driven Architecture**: Implements a file watcher to respond to file events (create, delete, modify) asynchronously.\\n- **Batch Processing**: Processes files in batches to optimize embedding operations.\\n\\nPrimary Abstractions and Data Structures:\\n- **FileMetadata**: Stores metadata about a file, including its path, last modified time, content hash, and processing status.\\n- **FileManagerConfig**: Configures the file manager, including batch size and ignored patterns.\\n- **FileManager**: Main class responsible for managing files, watching for changes, and processing files for embeddings.\\n\\nCore Algorithms and Techniques Implemented Here:\\n- **File Watching**: Monitors a directory for file events using `AsyncFileWatcher`.\\n- **File Processing**: Reads file contents, computes a hash, checks for changes, and processes files for embeddings if altered.\\n- **Batch Processing**: Gathers file paths into batches for efficient processing, improving performance.\\n\\nCode Flow:\\n1. **Initialization**: `FileManager` is initialized with a root path, configuration, and an embedding manager.\\n2. **Start Method**: Begins watching files and processes existing files and events asynchronously.\\n3. **Event Handling**: Handles file events (creation, deletion) and queues them for processing.\\n4. **Batch Processing Loop**: Processes files in batches, checking for changes and embedding new content when necessary.\\n5. **File Reading and Hashing**: Reads file content and computes a SHA-256 hash to detect changes.\\n6. **Embedding**: Uses the embedding manager to add vectors for the processed file chunks.\\n\\nImportant Control Structures and Their Purpose:\\n- **Locks**: `asyncio.Lock` is used to prevent concurrent processing of the same file.\\n- **Queues**: `asyncio.Queue` is used to manage file events and ensure orderly processing.\\n- **Try-Except Blocks**: Implemented throughout to handle and log errors during file operations.\\n\\nKey Function Interactions:\\n- `start`: Initiates the file watching and processing.\\n- `__handle_file_event`: Processes individual file events and updates the queue.\\n- `__process_file`: Reads, hashes, and embeds content from a file.\\n- `__embed`: Manages the embedding of file content into a vector store.\\n\\nDependencies:\\n- **External Libraries**:\\n  - `asyncio`: For asynchronous programming.\\n  - `aiofiles`: For asynchronous file I/O.\\n  - `hashlib`: For hashing file content.\\n  - `langchain_text_splitters`: For splitting text into chunks.\\n- **Internal Modules**:\\n  - `EmbeddingManager`: Provides methods for managing embeddings.\\n  - `AsyncFileWatcher`: Monitors file events asynchronously.\\n- No interfaces are explicitly implemented.\\n\\nImplementation Notes:\\n- **File Processing Logic**: The logic avoids reprocessing unchanged files by comparing hashes.\\n- **Batch Size Configuration**: Batch size can be adjusted via `FileManagerConfig`, allowing for flexibility based on performance needs.\\n- **Error Handling**: Comprehensive error logging ensures that issues are captured without crashing the application, facilitating easier debugging.\\n\\n[END_FILE]# [FILE]\\nFile Path: C:\\\\Users\\\\Utkarsh Priyadarshi\\\\WebstormProjects\\\\gep\\\\src\\\\vector\\\\embedding.py\\n\\n## Key Concepts:\\n- **Abstract Base Classes (ABC)**: The `EmbeddingProvider` is an abstract class that defines a common interface for embedding providers.\\n- **Asynchronous Programming**: The use of `async` and `await` enables non-blocking operations, particularly when fetching embeddings from external APIs.\\n- **Enum**: The `EmbeddingProviderType` enum is used to define different types of embedding providers in a clearer way.\\n\\n## Code Flow:\\n1. **Configuration**: The `EmbeddingConfig` class holds the configuration for the embedding models (like model name, API key, etc.).\\n2. **Provider Initialization**: `EmbeddingProvider` is an abstract class that requires concrete implementations to provide the `embed_documents` and `embed_query` methods.\\n3. **OpenAI Provider**: The `OpenAIEmbeddingProvider` inherits from `EmbeddingProvider` and implements the required methods using the `OpenAIEmbeddings` client.\\n4. **Embedding Retrieval**: The `embed_documents` method fetches embeddings in batches and handles exceptions with logging, while the `embed_query` method retrieves embeddings for a single query.\\n5. **Provider Creation**: The `create_embedding_provider` function takes a provider type and config to instantiate the corresponding embedding provider.\\n\\n## Dependencies:\\n- **External Libraries**: \\n  - `langchain_openai`: Provides the `OpenAIEmbeddings` class for fetching embeddings.\\n  - `tenacity`: For retrying operations that may fail due to transient errors.\\n- **Internal Modules**: None explicitly mentioned in this file.\\n\\n## Implementation Notes:\\n- **Error Handling**: The use of logging to track errors in embedding retrieval is crucial for debugging and monitoring.\\n- **Batch Processing**: The embedding of documents is done in batches to optimize performance and handle large lists efficiently.\\n- **Retry Logic**: The retry mechanism ensures robustness against transient issues when calling external APIs.\\n\\n[END_FILE]\\n\\n---\\n\\n# [FILE]\\nFile Path: C:\\\\Users\\\\Utkarsh Priyadarshi\\\\WebstormProjects\\\\gep\\\\src\\\\vector\\\\utils.py\\n\\n## Key Concepts:\\n- **File System Navigation**: The code navigates through the file system to find valid files based on specific patterns.\\n- **Multiprocessing**: Utilizes the `multiprocessing` module to read files concurrently, improving efficiency.\\n- **Pathlib**: The use of `Path` from `pathlib` simplifies path manipulations and makes the code more readable.\\n\\n## Code Flow:\\n1. **Directory Processing**: The `create_file_content_map` function starts by resolving the root directory and preparing to find files.\\n2. **File Discovery**: It uses `rglob` to recursively retrieve all files, ignoring directories.\\n3. **Validation**: Each file is validated against given ignore and accept patterns using the `_is_valid_file` function.\\n4. **File Reading**: Valid files are read in parallel using a pool of worker processes, where `_read_file` returns the file path and its content.\\n5. **Result Compilation**: The results are collected into a dictionary mapping file paths to their contents.\\n\\n## Dependencies:\\n- **External Libraries**: \\n  - `os`: For operating system-dependent functionality like retrieving CPU count.\\n  - `pathlib`: Provides an object-oriented way to handle filesystem paths.\\n- **Internal Modules**: None explicitly mentioned in this file.\\n\\n## Implementation Notes:\\n- **Concurrency**: The use of multiprocessing allows for faster file reading, which is beneficial when dealing with many files.\\n- **Pattern Matching**: The `_is_valid_file` function efficiently determines which files to process based on user-defined patterns.\\n- **Error Handling**: The code does not explicitly handle errors in file reading, which could be a potential improvement area.\\n\\n[END_FILE]\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.query import StreamQueryProcessor\n",
    "from src.query.prompt import PromptType, PromptProviderType\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "stream_query_processor = StreamQueryProcessor   (embedding_manager=embedding_manager,\n",
    "                                              llm=ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "\n",
    "\n",
    "final_result = \"\"\n",
    "\n",
    "async for result in stream_query_processor.stream_ananlysis(query=\"What is the purpose of the VectorStore class? and keep it short\", prompt_type=PromptType.FILE_WISE, prompt_provider=PromptProviderType.SEMANTIC):\n",
    "    final_result += str(result.content)\n",
    "\n",
    "\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result.md\", \"w\") as f:\n",
    "    f.write(final_result)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
