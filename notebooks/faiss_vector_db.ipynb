{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'src.vector.vector.FAISSVectorStore'> <class 'src.vector.vector.EmbeddingVector'> <function create_file_content_map at 0x0000024AFCF8AF20> <class 'src.vector.embedding.OpenAIEmbeddingProvider'>\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from src.vector import FAISSVectorStore, EmbeddingVector, create_file_content_map, OpenAIEmbeddingProvider, EmbeddingConfig\n",
    "print(FAISSVectorStore, EmbeddingVector, create_file_content_map, OpenAIEmbeddingProvider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 valid files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_map = create_file_content_map(os.path.join(\"..\", \"src\"), accept_patterns=[\"*.py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = EmbeddingConfig(model_name=\"text-embedding-3-small\",\n",
    "                         api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "provider = OpenAIEmbeddingProvider(config=config)\n",
    "\n",
    "embeddings = await provider.embed_documents(list(file_map.values()))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_vectors = [EmbeddingVector(id=file_path,\n",
    "                               vector=embedding,\n",
    "                               metadata={\"source\": file_path},\n",
    "                               text=file_map[file_path]) for file_path, embedding in zip(file_map.keys(), embeddings)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_store = FAISSVectorStore(dimension=1536, index_path=\"faiss_index.bin\")\n",
    "f_store.add_vectors(emb_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh Priyadarshi\\WebstormProjects\\gep\\src\\vector\\vector.py\n",
      "import os\n",
      "import faiss\n",
      "import numpy as np\n",
      "\n",
      "from abc import ABC, abstractmethod\n",
      "from dataclasses import dataclass\n",
      "from enum import Enum, auto\n",
      "from typing import Any, Optional\n",
      "\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker\n",
      "\n",
      "from langchain_core.embeddings import Embeddings\n",
      "from langchain_community.embeddings import OpenAIEmbeddings\n",
      "from langchain_community.vectorstores import Chroma, FAISS, PGVector, Pinecone\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class EmbeddingVector:\n",
      "    \"\"\"Represents an embedding vector with its metadata.\"\"\"\n",
      "    id: str\n",
      "    vector: list[float]\n",
      "    metadata: dict[str, Any]\n",
      "    text: str\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class SearchResult:\n",
      "    \"\"\"Represents a search result from the vector store.\"\"\"\n",
      "    text: str\n",
      "    metadata: dict[str, Any]\n",
      "    score: float\n",
      "    vector_id: str\n",
      "\n",
      "\n",
      "class VectorStoreType(Enum):\n",
      "    \"\"\"\n",
      "    Enum for the type of vector store.\n",
      "    \"\"\"\n",
      "    CHROMA = auto()\n",
      "    PGVECTOR = auto()\n",
      "    FAISS = auto()\n",
      "    PINECONE = auto()\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class VectorStoreConfig:\n",
      "    \"\"\"\n",
      "    Configuration for a vector store.\n",
      "    \"\"\"\n",
      "    store_type: VectorStoreType\n",
      "    connection_params: dict[str, Any]\n",
      "    embedding_model: Optional[Embeddings] = None\n",
      "\n",
      "    def __post_init__(self):\n",
      "        if self.embedding_model is None:\n",
      "            self.embedding_model = OpenAIEmbeddings()\n",
      "\n",
      "\n",
      "class VectorStore(ABC):\n",
      "    \"\"\"\n",
      "    Abstract base class for a vector store.\n",
      "    \"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def add_vectors(self, vectors: list[EmbeddingVector]) -> None:\n",
      "        \"\"\"Add vectors to the vector store.\"\"\"\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def query(self, query_vector: list[float], k: int = 10, filter: Optional[dict] = None) -> list[SearchResult]:\n",
      "        \"\"\"Query the vector store.\"\"\"\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def delete(self, ids: list[str]) -> None:\n",
      "        \"\"\"Delete vectors from the vector store.\"\"\"\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def clear(self) -> None:\n",
      "        \"\"\"Clear all vectors from the store.\"\"\"\n",
      "        pass\n",
      "\n",
      "\n",
      "class ChromaVectorStore(VectorStore):\n",
      "    \"\"\"\n",
      "    Vector store using Chroma.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, persist_dir: str, embedding_model: Optional[Embeddings] = None):\n",
      "        self.persist_dir = persist_dir\n",
      "        self.embedding_model = embedding_model or OpenAIEmbeddings()\n",
      "        self.store = Chroma(persist_directory=persist_dir,\n",
      "                            embedding_function=self.embedding_model)\n",
      "\n",
      "    def add_vectors(self, vectors: list[EmbeddingVector]) -> None:\n",
      "        \"\"\"Add vectors to the Chroma store.\"\"\"\n",
      "        try:\n",
      "            documents = [v.text for v in vectors]\n",
      "            metadatas = [v.metadata for v in vectors]\n",
      "            embeddings = [v.vector for v in vectors]\n",
      "            ids = [v.id for v in vectors]\n",
      "\n",
      "            self.store.add_documents(\n",
      "                documents=documents,\n",
      "                metadatas=metadatas,\n",
      "                embeddings=embeddings,\n",
      "                ids=ids\n",
      "            )\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Failed to add vectors to Chroma: {e}\")\n",
      "\n",
      "    def clear(self) -> None:\n",
      "        \"\"\"Clear all vectors from the Chroma store.\"\"\"\n",
      "        try:\n",
      "            self.store.delete_collection()\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Failed to clear Chroma store: {e}\")\n",
      "\n",
      "    def delete(self, ids: list[str]) -> None:\n",
      "        \"\"\"Delete vectors from the Chroma store.\"\"\"\n",
      "        try:\n",
      "            self.store.delete(ids)\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Failed to delete vectors from Chroma: {e}\")\n",
      "\n",
      "\n",
      "class FAISSVectorStore(VectorStore):\n",
      "    \"\"\"\n",
      "    Vector store using FAISS.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, dimension: int, index_path: str, embedding_model: Optional[Embeddings] = None) -> None:\n",
      "        self.dimension = dimension\n",
      "        self.index_path = index_path\n",
      "        self.embedding_model = embedding_model or OpenAIEmbeddings()\n",
      "\n",
      "        self.index = faiss.IndexFlatL2(dimension)\n",
      "        self.id_map: dict[str, int] = {}\n",
      "        self.vectors: list[EmbeddingVector] = []\n",
      "        self.__load_index()\n",
      "\n",
      "    def query(self, query_vector: list[float], k: int = 10, filter: Optional[dict] = None) -> list[SearchResult]:\n",
      "        \"\"\"Query the FAISS store and return the top k results.\"\"\"\n",
      "        try:\n",
      "\n",
      "            query_arr = np.array([query_vector], dtype=np.float32)\n",
      "            distances, indices = self.index.search(query_arr, k)\n",
      "\n",
      "            results = []\n",
      "            for distance, idx in zip(distances[0], indices[0]):\n",
      "                if idx < 0 or idx >= len(self.vectors):\n",
      "                    continue\n",
      "\n",
      "                vector = self.vectors[idx]\n",
      "\n",
      "                results.append(SearchResult(\n",
      "                    text=vector.text,\n",
      "                    metadata=vector.metadata,\n",
      "                    score=float(distance),\n",
      "                    vector_id=vector.id\n",
      "                ))\n",
      "\n",
      "            return results\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Failed to query FAISS: {str(e)}\") from e\n",
      "\n",
      "    def __load_index(self) -> None:\n",
      "        \"\"\"Load the FAISS index from disk if it exists.\"\"\"\n",
      "        try:\n",
      "            if os.path.exists(self.index_path):\n",
      "                self.index = faiss.read_index(self.index_path)\n",
      "        except Exception as e:\n",
      "            print(f\"Warning: Failed to load FAISS index: {e}\")\n",
      "\n",
      "    def __save_index(self) -> None:\n",
      "        \"\"\"Save the FAISS index to disk.\"\"\"\n",
      "        try:\n",
      "            faiss.write_index(self.index, self.index_path)\n",
      "        except Exception as e:\n",
      "            print(f\"Warning: Failed to save FAISS index: {e}\")\n",
      "\n",
      "    def add_vectors(self, vectors: list[EmbeddingVector]) -> None:\n",
      "        \"\"\"Add vectors to the FAISS store.\"\"\"\n",
      "        try:\n",
      "            if not vectors:\n",
      "                return\n",
      "\n",
      "            vec_arr = np.array(\n",
      "                [v.vector for v in vectors], dtype=np.float32)\n",
      "\n",
      "            start_idx = len(self.vectors)\n",
      "            self.index.add(vec_arr)\n",
      "\n",
      "            for i, vector in enumerate(vectors):\n",
      "                self.id_map[vector.id] = start_idx + i\n",
      "                self.vectors.append(vector)\n",
      "\n",
      "            self.__save_index()\n",
      "\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Failed to add vectors to FAISS: {e}\")\n",
      "\n",
      "    def delete(self, ids: list[str]) -> None:\n",
      "        \"\"\"Delete vectors from the FAISS store.\"\"\"\n",
      "        try:\n",
      "            indices_to_delete = [self.id_map[id] for id in ids]\n",
      "            if not indices_to_delete:\n",
      "                return\n",
      "\n",
      "            new_vectors = []\n",
      "            new_id_map = {}\n",
      "\n",
      "            for i, vector in enumerate(self.vectors):\n",
      "                if i not in indices_to_delete:\n",
      "                    new_id_map[vector.id] = len(new_vectors)\n",
      "                    new_vectors.append(vector)\n",
      "\n",
      "            # Rebuild the index with the new vectors\n",
      "            self.index = faiss.IndexFlatL2(self.dimension)\n",
      "            if new_vectors:\n",
      "                vectors_array = np.array(\n",
      "                    [v.vector for v in new_vectors], dtype=np.float32)\n",
      "                self.index.add(vectors_array)\n",
      "\n",
      "            self.vectors = new_vectors\n",
      "            self.id_map = new_id_map\n",
      "\n",
      "            self.__save_index()\n",
      "\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Failed to delete vectors from FAISS: {e}\")\n",
      "\n",
      "    def clear(self) -> None:\n",
      "        \"\"\"Clear all vectors from the FAISS store.\"\"\"\n",
      "        try:\n",
      "            self.index = faiss.IndexFlatL2(self.dimension)\n",
      "            self.id_map = {}\n",
      "            self.vectors = []\n",
      "            self.__save_index()\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Failed to clear FAISS store: {e}\")\n",
      "\n",
      "\n",
      "class PGVectorStore(VectorStore):\n",
      "    \"\"\"\n",
      "    Vector store using PGVector.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, connection_str: str, table_name: str = \"embeddings\", embedding_model: Optional[Embeddings] = None):\n",
      "        self.connection_str = connection_str\n",
      "        self.table_name = table_name\n",
      "        self.embedding_model = embedding_model or OpenAIEmbeddings()\n",
      "\n",
      "        self.engine = create_engine(connection_str)\n",
      "        self.session = sessionmaker(bind=self.engine)\n",
      "\n",
      "        with self.engine.connect() as conn:\n",
      "\n",
      "            # create table if not exists\n",
      "            conn.execute(f\"\"\"\n",
      "                CREATE TABLE IF NOT EXISTS {self.table_name} (\n",
      "                    id TEXT PRIMARY KEY,\n",
      "                    vector vector({self.embedding_model.dimension}),\n",
      "                    text TEXT,\n",
      "                    metadata JSONB,\n",
      "                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n",
      "                )\n",
      "            \"\"\")\n",
      "\n",
      "            conn.execute(f\"\"\"\n",
      "                CREATE INDEX IF NOT EXISTS {self.table_name}_vector_idx\n",
      "                ON {self.table_name}\n",
      "                USING ivfflat (vector vector_cosine_ops)\n",
      "            \"\"\")\n",
      "\n",
      "    def _initialize_store(self) -> PGVector:\n",
      "        return PGVector(embedding=self.config.embedding_model)\n",
      "\n",
      "C:\\Users\\Utkarsh Priyadarshi\\WebstormProjects\\gep\\src\\vector\\manager.py\n",
      "import time\n",
      "from typing import Optional\n",
      "\n",
      "\n",
      "from .embedding import EmbeddingProvider\n",
      "from .vector import VectorStore, EmbeddingVector\n",
      "\n",
      "\n",
      "class EmbeddingManager:\n",
      "    \"\"\"\n",
      "    Manages the coordination between vector stores and embedding providers.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, embedding_provider: EmbeddingProvider, vector_store: VectorStore, batch_size: int = 100) -> None:\n",
      "        self.embedding_provider = embedding_provider\n",
      "        self.batch_size = batch_size\n",
      "        self.vector_store = vector_store\n",
      "\n",
      "    async def add_texts(self, texts: list[str], metadatas: Optional[list[dict]] = None) -> None:\n",
      "        \"\"\"\n",
      "        Add texts to the vector store.\n",
      "        \"\"\"\n",
      "\n",
      "        try:\n",
      "            embeddings = await self.embedding_provider.embed_documents(texts)\n",
      "            vectors = []\n",
      "\n",
      "            for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
      "                md = metadatas[i] if metadatas else {}\n",
      "                vector = EmbeddingVector(\n",
      "                    id=f\"vector_{time.time()}_{i}\",\n",
      "                    vector=embedding,\n",
      "                    metadata=md,\n",
      "                    text=text,\n",
      "                )\n",
      "\n",
      "                vectors.append(vector)\n",
      "\n",
      "            await self.vector_store.add_vectors(vectors)\n",
      "        except Exception as e:\n",
      "            print(f\"Error adding texts to vector store: {e}\")\n",
      "            raise e\n",
      "\n",
      "    async def delete_vectors(self, ids: list[str]) -> None:\n",
      "        \"\"\"\n",
      "        Delete vectors from the vector store.\n",
      "        \"\"\"\n",
      "        await self.vector_store.delete_vectors(ids)\n",
      "\n",
      "    async def similarity_search(self, query: str, limit: int = 10, filter: Optional[dict] = None) -> list[EmbeddingVector]:\n",
      "        \"\"\"\n",
      "        Perform a similarity search on the vector store.\n",
      "        \"\"\"\n",
      "\n",
      "        try:\n",
      "            query_embedding = await self.embedding_provider.embed_query(query)\n",
      "            return await self.vector_store.query(\n",
      "                query_vector=query_embedding,\n",
      "                k=limit,\n",
      "                filter=filter,\n",
      "            )\n",
      "        except Exception as e:\n",
      "            print(f\"Error performing similarity search: {e}\")\n",
      "            raise e\n",
      "\n",
      "C:\\Users\\Utkarsh Priyadarshi\\WebstormProjects\\gep\\src\\vector\\__init__.py\n",
      "from .manager import EmbeddingManager, EmbeddingProvider\n",
      "from .embedding import EmbeddingConfig, create_embedding_provider, EmbeddingProviderType, OpenAIEmbeddingProvider\n",
      "from .vector import VectorStore, VectorStoreConfig, VectorStoreType, ChromaVectorStore, PGVectorStore, FAISSVectorStore, EmbeddingVector\n",
      "from .file import FileMetadata, FileEvent, EventType, FileManager, FileManagerConfig\n",
      "from .utils import create_file_content_map\n",
      "\n",
      "\n",
      "__all__ = [\n",
      "    \"EmbeddingManager\",\n",
      "    \"EmbeddingProvider\",\n",
      "    \"EmbeddingConfig\",\n",
      "    \"create_embedding_provider\",\n",
      "    \"EmbeddingProviderType\",\n",
      "    \"OpenAIEmbeddingProvider\",\n",
      "    \"VectorStore\",\n",
      "    \"VectorStoreConfig\",\n",
      "    \"VectorStoreType\",\n",
      "    \"ChromaVectorStore\",\n",
      "    \"FileMetadata\",\n",
      "    \"FileEvent\",\n",
      "    \"EventType\",\n",
      "    \"FileManager\",\n",
      "    \"FileManagerConfig\",\n",
      "    \"PGVectorStore\",\n",
      "    \"FAISSVectorStore\",\n",
      "    \"EmbeddingVector\",\n",
      "    \"create_file_content_map\",\n",
      "]\n",
      "\n",
      "C:\\Users\\Utkarsh Priyadarshi\\WebstormProjects\\gep\\src\\cmd\\command.py\n",
      "import rich_click as click\n",
      "from rich.console import Console\n",
      "from rich.panel import Panel\n",
      "from rich.progress import track\n",
      "from vector import EmbeddingManager, EmbeddingProvider, VectorStoreType, EmbeddingProviderType\n",
      "from query import QueryProcessor, QueryType\n",
      "\n",
      "\n",
      "# Configure rich-click\n",
      "click.rich_click.USE_RICH_MARKUP = True\n",
      "click.rich_click.USE_MARKDOWN = True\n",
      "click.rich_click.SHOW_ARGUMENTS = True\n",
      "console = Console()\n",
      "\n",
      "\n",
      "vector_stores = [store.name.lower() for store in VectorStoreType]\n",
      "embedding_providers = [provider.name.lower()\n",
      "                       for provider in EmbeddingProviderType]\n",
      "\n",
      "\n",
      "@click.group()\n",
      "def cli():\n",
      "    \"\"\"\n",
      "    [bold blue]Welcome to Your Application[/bold blue]\n",
      "\n",
      "    A CLI tool for processing queries and managing embeddings.\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "\n",
      "@cli.command()\n",
      "@click.argument('query', required=True)\n",
      "@click.option('--type', '-t', type=click.Choice(['text', 'image']), default='text', help='Type of query to process')\n",
      "def process_query(query: str, type: str):\n",
      "    \"\"\"\n",
      "    Process a query and return results.\n",
      "\n",
      "    Arguments:\n",
      "        query: The query string to process\n",
      "    \"\"\"\n",
      "    with console.status(\"[bold green]Processing query...\"):\n",
      "        processor = QueryProcessor()\n",
      "        result = processor.process(query)\n",
      "        console.print(Panel(f\"Result: {result}\", title=\"Query Result\"))\n",
      "\n",
      "\n",
      "@cli.command()\n",
      "@click.option('--provider', '-p',\n",
      "              type=click.Choice(['openai', 'local']),\n",
      "              default='openai',\n",
      "              help='Select the embedding provider')\n",
      "@click.option('--create/--no-create',\n",
      "              default=False,\n",
      "              help='Create new embeddings')\n",
      "def manage_embeddings(provider: str, create: bool):\n",
      "    \"\"\"\n",
      "    Manage embedding operations.\n",
      "\n",
      "    Options:\n",
      "        --provider: The embedding provider to use\n",
      "        --create: Whether to create new embeddings\n",
      "    \"\"\"\n",
      "    console.print(f\"\\n[bold]Embedding Management - Using {provider}[/bold]\")\n",
      "\n",
      "    if create:\n",
      "        for step in track(range(100), description=\"Creating embeddings...\"):\n",
      "            # Your embedding creation logic here\n",
      "            pass\n",
      "        console.print(\"[green]Embeddings updated successfully![/green]\")\n",
      "\n",
      "\n",
      "@cli.command()\n",
      "@click.option('--verbose', '-v', is_flag=True, help='Enable verbose output')\n",
      "def status(verbose: bool):\n",
      "    \"\"\"Check the status of the application.\"\"\"\n",
      "    with console.status(\"[bold blue]Checking status...\"):\n",
      "        # Add your status check logic here\n",
      "        status_msg = \"All systems operational\"\n",
      "        console.print(Panel(status_msg, title=\"Status Check\"))\n",
      "\n",
      "        if verbose:\n",
      "            # Add detailed status information\n",
      "            console.print(\"[dim]Detailed status information...[/dim]\")\n",
      "\n",
      "\n",
      "@cli.command()\n",
      "@click.option('--root-path', '-r', type=click.Path(exists=True), default='.', help='The root path to watch')\n",
      "@click.option('--ignored-patterns', '-i', type=list, default=[], help='Patterns to ignore')\n",
      "@click.option('--batch-size', '-b', type=int, default=10, help='Batch size for processing files')\n",
      "@click.option('--embedding-provider', '-e', type=click.Choice(embedding_providers), default='openai', help='The embedding provider to use')\n",
      "@click.option('--vector-store', '-v', type=click.Choice(vector_stores), default='chroma', help='The vector store to use')\n",
      "def init(root_path: str, ignored_patterns: list[str], batch_size: int, embedding_provider: str, vector_store: str):\n",
      "    \"\"\"\n",
      "    Initialize the application.\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "C:\\Users\\Utkarsh Priyadarshi\\WebstormProjects\\gep\\src\\query\\query.py\n",
      "from dataclasses import dataclass\n",
      "from typing import Any, Optional\n",
      "from enum import Enum, auto\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain_core.prompts import PromptTemplate\n",
      "\n",
      "from vector.manager import EmbeddingManager\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class QueryResult:\n",
      "    source_files: list[str]\n",
      "    relevant_texts: list[str]\n",
      "    analysis: str\n",
      "    relevance_scores: list[float]\n",
      "    metadata: dict[str, Any]\n",
      "\n",
      "\n",
      "class QueryType(Enum):\n",
      "    SEMANTIC = auto()\n",
      "    CODE_PATTERN = auto()\n",
      "    SECURITY = auto()\n",
      "    CODE_CHANGE = auto()\n",
      "\n",
      "\n",
      "class QueryProcessor:\n",
      "    \"\"\"\n",
      "    Handles query processing and analysis for the embedding system.\n",
      "    Works with the IntegratedEmbeddingManager to search and analyze code.\n",
      "\n",
      "    Query Text → Query Embedding → Vector Store Search → Filter Results → LLM Analysis\n",
      "            (embedding model)   (similarity search)  (score-based)   (explanation)\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, embedding_manager: EmbeddingManager, llm: ChatOpenAI, max_results: int = 10, min_relevance_score: float = 0.5) -> None:\n",
      "        self.embedding_manager = embedding_manager\n",
      "        self.llm = llm\n",
      "        self.max_results = max_results\n",
      "        self.min_relevance_score = min_relevance_score\n",
      "        self.prompts = self.__init_prompts()\n",
      "\n",
      "    def __init_prompts(self) -> dict[QueryType, PromptTemplate]:\n",
      "        \"\"\"\n",
      "        Creates specialized prompts for different types of queries.\n",
      "        Each prompt is designed to guide the AI in analyzing code in a specific way.\n",
      "        \"\"\"\n",
      "\n",
      "        return {\n",
      "            QueryType.SEMANTIC: PromptTemplate(\n",
      "                input_variables=[\"query\", \"code_contexts\"],\n",
      "                template=\"\"\"\n",
      "                Analyze these code sections for relevance to the query.\n",
      "                Query: {query}\n",
      "                \n",
      "                Code Contexts:\n",
      "                {code_contexts}\n",
      "                \n",
      "                Provide a detailed analysis including:\n",
      "                1. How each section relates to the query\n",
      "                2. Key implementation details\n",
      "                3. Any notable patterns or practices\n",
      "                4. Suggestions for understanding or using this code\n",
      "                \"\"\"\n",
      "            ),\n",
      "            QueryType.CODE_PATTERN: PromptTemplate(\n",
      "                input_variables=[\"query\", \"code_contexts\"],\n",
      "                template=\"\"\"\n",
      "                Analyze these code implementations for the requested pattern.\n",
      "                Query: {query}\n",
      "                \n",
      "                Code Contexts:\n",
      "                {code_contexts}\n",
      "                \n",
      "                Provide a technical analysis covering:\n",
      "                1. Implementation patterns used\n",
      "                2. Function signatures and interfaces\n",
      "                3. Error handling approaches\n",
      "                4. Dependencies and coupling\n",
      "                5. Potential optimizations\n",
      "                \"\"\"\n",
      "            ),\n",
      "        }\n",
      "\n",
      "    async def process_query(self, query: str, query_type: QueryType, filters: Optional[list[str]] = None) -> QueryResult:\n",
      "        \"\"\"\n",
      "        Process a query and return analyzed results.\n",
      "\n",
      "        This method coordinates between the vector store search and AI analysis\n",
      "        to provide comprehensive results.\n",
      "        \"\"\"\n",
      "\n",
      "        try:\n",
      "\n",
      "            search_results = self.embedding_manager.similarity_search(\n",
      "                query=query,\n",
      "                limit=self.max_results,\n",
      "                filter=filters,\n",
      "            )\n",
      "\n",
      "            filtered_results = [\n",
      "                (text, score, metadata) for text, score, metadata in search_results if score >= self.min_relevance_score\n",
      "            ]\n",
      "\n",
      "            if not filtered_results:\n",
      "                return QueryResult(\n",
      "                    source_files=[],\n",
      "                    relevant_texts=[],\n",
      "                    analysis=\"No relevant results found.\",\n",
      "                    relevance_scores=[],\n",
      "                    metadata={},\n",
      "                )\n",
      "\n",
      "            code_contexts = \"\\n\\n\".join([\n",
      "                f\"File: {meta['source']}\\n\"\n",
      "                f\"Content:\\n{text}\\n\"\n",
      "                for text, _, meta in filtered_results\n",
      "            ])\n",
      "            prompt = self.prompts[query_type]\n",
      "            analysis_prompt = prompt.format(\n",
      "                query=query,\n",
      "                code_contexts=code_contexts\n",
      "            )\n",
      "\n",
      "            llm_response = await self.llm.ainvoke(analysis_prompt)\n",
      "\n",
      "            return QueryResult(\n",
      "                source_files=[meta['source']\n",
      "                              for _, _, meta in filtered_results],\n",
      "                relevant_texts=[text for text, _, _ in filtered_results],\n",
      "                analysis=llm_response,\n",
      "                relevance_scores=[score for _, score, _ in filtered_results],\n",
      "                metadata={meta: meta for _, _, meta in filtered_results},\n",
      "            )\n",
      "\n",
      "        except Exception as e:\n",
      "            print(f\"Error processing query: {e}\")\n",
      "            raise e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_vector = await provider.embed_query(\"how are we implementing the vector database\")\n",
    "query_results = f_store.query(query_vector=query_vector, k=5)\n",
    "\n",
    "\n",
    "for result in query_results:\n",
    "    print(result.metadata[\"source\"])\n",
    "    print(result.text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
